# Object Manipulation Integration Plan for RoboCrew

## Executive Summary

This document outlines a comprehensive plan for integrating object manipulation capabilities into RoboCrew, enabling robots to interact with their environment through arm control and object handling. The integration will leverage Vision-Language-Action (VLA) models and the existing lerobot motor control infrastructure.

## Table of Contents

1. [Current State Analysis](#current-state-analysis)
2. [Architecture Overview](#architecture-overview)
3. [Component Design](#component-design)
4. [Integration Phases](#integration-phases)
5. [Safety Considerations](#safety-considerations)
6. [Testing Strategy](#testing-strategy)
7. [Future Enhancements](#future-enhancements)

---

## Current State Analysis

### Existing Infrastructure

**Strengths:**
- âœ… LLM agent framework with tool-based architecture
- âœ… lerobot motor control library already integrated
- âœ… Feetech servo motor support (STS3215)
- âœ… Vision system with camera feed and spatial understanding
- âœ… Multi-bus motor control (separate buses for wheels, head, and potentially arms)
- âœ… Tool creation pattern established (`create_move_forward`, etc.)
- âœ… Async and sync tool execution support

**Current Capabilities:**
- Wheel-based navigation (forward/backward, turn left/right)
- Head movement (yaw and pitch control)
- Camera integration with angle grid overlay
- Voice-activated task commands

**Gaps for Object Manipulation:**
- No arm motor control utilities
- No VLA model integration
- No gripper control mechanisms
- No object detection/tracking system
- No manipulation-specific tools
- No inverse kinematics utilities

---

## Architecture Overview

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       LLM Agent Core                         â”‚
â”‚  (Task Planning, Tool Selection, Multi-modal Understanding)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                     â”‚
          â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Movement Tools  â”‚   â”‚ Manipulation     â”‚
â”‚  (Existing)      â”‚   â”‚ Tools (New)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ move_forward   â”‚   â”‚ â€¢ reach_object   â”‚
â”‚ â€¢ turn_left      â”‚   â”‚ â€¢ grasp_object   â”‚
â”‚ â€¢ turn_right     â”‚   â”‚ â€¢ release_object â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â€¢ move_arm_to    â”‚
         â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚
         â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Wheel Controller â”‚   â”‚   Arm Controller         â”‚
â”‚ (XLeRobotWheels) â”‚   â”‚   (XLeRobotArms - New)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚
         â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚             â”‚                  â”‚
         â–¼             â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Wheel Motors â”‚  â”‚ Arm      â”‚    â”‚ VLA Model    â”‚
â”‚ (Bus 1)      â”‚  â”‚ Motors   â”‚    â”‚ Integration  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ (Bus 2+3)â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Principles

1. **Modularity**: Arm control should be a separate module like wheel control
2. **Consistency**: Follow existing patterns (tool creation, bus management)
3. **Safety**: Built-in collision detection and movement limits
4. **Flexibility**: Support both low-level motor control and high-level VLA actions
5. **Extensibility**: Easy to add new manipulation primitives

---

## Component Design

### 1. Arm Controller Module

**Location**: `src/robocrew/robots/XLeRobot/arm_controls.py`

**Purpose**: Low-level arm motor control, similar to `wheel_controls.py`

**Key Features:**
```python
class XLeRobotArms:
    """
    Controller for XLeRobot arm motors with support for:
    - Dual arm control (left and right arms)
    - Individual joint control
    - End-effector positioning
    - Gripper control
    - Collision detection
    - Joint limit enforcement
    """
    
    def __init__(
        self,
        left_arm_usb: str = "/dev/arm_left",
        right_arm_usb: str = "/dev/arm_right",
        motor_config: Optional[Dict] = None
    ):
        # Initialize motor buses for both arms
        # Set up motor configurations
        # Initialize gripper controls
        pass
    
    # Joint-level control
    def set_joint_positions(self, arm: str, positions: Dict[int, float]) -> None
    def get_joint_positions(self, arm: str) -> Dict[int, float]
    def set_gripper_state(self, arm: str, open_percentage: float) -> None
    
    # Arm-level control
    def move_to_position(self, arm: str, x: float, y: float, z: float) -> bool
    def move_relative(self, arm: str, dx: float, dy: float, dz: float) -> bool
    
    # Safety features
    def check_collision(self, positions: Dict[int, float]) -> bool
    def within_limits(self, positions: Dict[int, float]) -> bool
    def emergency_stop(self) -> None
```

**Motor Mapping** (Example for XLeRobot):
```python
LEFT_ARM_MOTORS = {
    1: "shoulder_pan",   # Base rotation
    2: "shoulder_lift",  # Shoulder up/down
    3: "elbow",          # Elbow bend
    4: "wrist_rotate",   # Wrist rotation
    5: "wrist_tilt",     # Wrist tilt
    6: "gripper",        # Gripper open/close
}

RIGHT_ARM_MOTORS = {
    # Similar mapping for right arm
}
```

### 2. VLA Model Integration

**Location**: `src/robocrew/models/vla.py`

**Purpose**: Interface with Vision-Language-Action models for high-level manipulation

**Approach Options:**

#### Option A: OpenVLA Integration
```python
class VLAController:
    """
    Integration with OpenVLA or similar models for learned manipulation.
    Uses pre-trained policies for common manipulation tasks.
    """
    
    def __init__(
        self,
        model_name: str = "openvla-7b",
        device: str = "cuda"
    ):
        # Load VLA model
        # Initialize processor
        pass
    
    def predict_action(
        self,
        image: np.ndarray,
        instruction: str,
        current_state: Dict
    ) -> Dict[str, float]:
        """
        Predict robot action given visual input and language instruction.
        Returns: Joint positions or end-effector target
        """
        pass
```

#### Option B: RT-X/RT-2 Style Integration
```python
class RTXController:
    """
    Integration with RT-X family models via their API or local deployment.
    """
    
    def get_manipulation_action(
        self,
        observation: Dict,
        task_description: str
    ) -> np.ndarray:
        # Returns action vector for robot control
        pass
```

#### Option C: LeRobot Policy Integration
```python
class LeRobotPolicy:
    """
    Use lerobot's built-in policy framework for manipulation.
    Can load pre-trained policies or train custom ones.
    """
    
    def __init__(self, policy_path: str):
        from lerobot.common.policies import load_policy
        self.policy = load_policy(policy_path)
    
    def get_action(self, observation: Dict) -> np.ndarray:
        return self.policy.select_action(observation)
```

**Recommended Approach**: Start with Option C (LeRobot Policy) as it's already integrated, then add Option A (OpenVLA) for more general capabilities.

### 3. Manipulation Tools

**Location**: `src/robocrew/robots/XLeRobot/manipulation_tools.py`

**Purpose**: High-level LangChain tools for object manipulation

**Tool Categories:**

#### A. Basic Manipulation Tools
```python
def create_reach_object(arm_controller, vla_controller=None):
    @tool
    def reach_object(
        object_name: str,
        arm: str = "right",
        approach_angle: float = 0.0
    ) -> str:
        """
        Reach toward the specified object with the given arm.
        
        Args:
            object_name: Name or description of target object
            arm: Which arm to use ('left' or 'right')
            approach_angle: Angle to approach from (degrees)
        
        Returns:
            Status message indicating success/failure
        """
        # 1. Get current camera view
        # 2. If VLA available, use it to plan approach
        # 3. Move arm to pre-grasp position
        # 4. Return status
        pass
    
    return reach_object

def create_grasp_object(arm_controller):
    @tool
    def grasp_object(arm: str = "right", grip_force: float = 0.5) -> str:
        """
        Close gripper to grasp object.
        
        Args:
            arm: Which arm's gripper to use
            grip_force: Gripping force (0.0 to 1.0)
        """
        pass
    
    return grasp_object

def create_release_object(arm_controller):
    @tool
    def release_object(arm: str = "right") -> str:
        """Release currently grasped object by opening gripper."""
        pass
    
    return release_object
```

#### B. Compound Manipulation Tools
```python
def create_pick_and_place(arm_controller, vla_controller):
    @tool
    def pick_and_place(
        object_name: str,
        target_location: str,
        arm: str = "right"
    ) -> str:
        """
        Pick up an object and place it at target location.
        
        This is a compound action that:
        1. Identifies the object
        2. Plans approach trajectory
        3. Grasps the object
        4. Moves to target location
        5. Releases the object
        """
        pass
    
    return pick_and_place

def create_handover_object(arm_controller):
    @tool
    def handover_object(
        object_name: str,
        from_arm: str = "right",
        to_arm: str = "left"
    ) -> str:
        """Transfer object from one arm to the other."""
        pass
    
    return handover_object
```

#### C. Utility Tools
```python
def create_reset_arm(arm_controller):
    @tool
    def reset_arm(arm: str = "both") -> str:
        """Move arm(s) to safe default position."""
        pass
    
    return reset_arm

def create_check_grasp(arm_controller):
    @tool
    def check_grasp(arm: str = "right") -> str:
        """Check if gripper is currently holding an object."""
        pass
    
    return check_grasp
```

### 4. Object Detection Integration

**Location**: `src/robocrew/vision/object_detection.py`

**Purpose**: Identify and locate objects in camera view for manipulation

```python
class ObjectDetector:
    """
    Object detection and localization for manipulation tasks.
    """
    
    def __init__(
        self,
        model_name: str = "yolov8n",
        use_depth: bool = False
    ):
        # Initialize detection model
        # Setup depth estimation if enabled
        pass
    
    def detect_objects(
        self,
        image: np.ndarray,
        classes: Optional[List[str]] = None
    ) -> List[Detection]:
        """
        Detect objects in image.
        Returns list of detections with bounding boxes and confidence.
        """
        pass
    
    def estimate_3d_position(
        self,
        detection: Detection,
        camera_params: Dict
    ) -> Tuple[float, float, float]:
        """
        Estimate 3D position of object relative to robot base.
        Uses depth estimation or stereo vision if available.
        """
        pass
```

**Integration with LLM Agent:**
- Add object detection to the agent's perception pipeline
- Include detected objects in the visual context sent to LLM
- Use spatial information for manipulation planning

### 5. Inverse Kinematics Module

**Location**: `src/robocrew/robots/XLeRobot/kinematics.py`

**Purpose**: Convert end-effector positions to joint angles

```python
class ArmKinematics:
    """
    Forward and inverse kinematics for XLeRobot arms.
    """
    
    def __init__(self, arm_config: Dict):
        # Load DH parameters or URDF
        # Initialize IK solver
        pass
    
    def forward_kinematics(
        self,
        joint_angles: Dict[int, float]
    ) -> Tuple[float, float, float]:
        """
        Calculate end-effector position from joint angles.
        Returns: (x, y, z) in robot base frame
        """
        pass
    
    def inverse_kinematics(
        self,
        target_x: float,
        target_y: float,
        target_z: float,
        current_joints: Optional[Dict] = None
    ) -> Optional[Dict[int, float]]:
        """
        Calculate joint angles to reach target position.
        Returns joint angles or None if unreachable.
        """
        pass
    
    def check_reachability(
        self,
        x: float, y: float, z: float
    ) -> bool:
        """Check if target position is within workspace."""
        pass
```

---

## Integration Phases

### Phase 1: Foundation (Weeks 1-2)

**Goal**: Establish basic arm control infrastructure

**Tasks:**
1. âœ… Create `arm_controls.py` module
   - Implement `XLeRobotArms` class
   - Add motor bus initialization for arms
   - Implement basic joint position control
   - Add gripper control

2. âœ… Create arm configuration
   - Define motor IDs for left/right arms
   - Set joint limits and safety parameters
   - Create configuration JSON file

3. âœ… Test basic arm movements
   - Verify motor communication
   - Test individual joint control
   - Validate gripper operation
   - Ensure safety limits work

**Deliverables:**
- Working `XLeRobotArms` class
- Configuration file: `arm_config.json`
- Basic test script for arm movements

### Phase 2: Tool Integration (Weeks 3-4)

**Goal**: Create LangChain tools for basic manipulation

**Tasks:**
1. âœ… Implement basic manipulation tools
   - `create_move_arm_to()` - Move arm to position
   - `create_open_gripper()` - Open gripper
   - `create_close_gripper()` - Close gripper
   - `create_reset_arm()` - Return to home position

2. âœ… Integrate with LLMAgent
   - Add manipulation tools to agent initialization
   - Update example scripts
   - Test tool invocation from LLM

3. âœ… Update documentation
   - Add manipulation examples to README
   - Document tool parameters
   - Create usage guide

**Deliverables:**
- Set of working manipulation tools
- Updated example: `examples/manipulation_demo.py`
- Updated README with manipulation section

### Phase 3: VLA Model Integration (Weeks 5-8)

**Goal**: Add intelligent manipulation with VLA models

**Tasks:**
1. âœ… Implement VLA controller
   - Choose VLA model (OpenVLA or LeRobot policy)
   - Create model wrapper class
   - Implement action prediction interface

2. âœ… Create VLA-enhanced tools
   - `create_reach_object()` - VLA-guided reaching
   - `create_grasp_object()` - Intelligent grasping
   - `create_pick_and_place()` - Complete pick-place action

3. âœ… Add object detection
   - Integrate object detection model
   - Add detection to perception pipeline
   - Connect detection with VLA input

4. âœ… Testing and refinement
   - Test with common household objects
   - Tune VLA model performance
   - Collect failure cases for improvement

**Deliverables:**
- Working VLA integration
- Object detection system
- Advanced manipulation tools
- Performance benchmarks

### Phase 4: Kinematics and Planning (Weeks 9-10)

**Goal**: Add trajectory planning and collision avoidance

**Tasks:**
1. âœ… Implement kinematics
   - Forward kinematics calculator
   - Inverse kinematics solver
   - Workspace boundary checking

2. âœ… Add motion planning
   - Trajectory interpolation
   - Basic collision detection
   - Smooth motion profiles

3. âœ… Safety enhancements
   - Joint velocity limits
   - Emergency stop mechanism
   - Collision avoidance

**Deliverables:**
- Kinematics module
- Motion planning utilities
- Safety system

### Phase 5: Polish and Documentation (Weeks 11-12)

**Goal**: Finalize integration and create comprehensive documentation

**Tasks:**
1. âœ… Comprehensive testing
   - Unit tests for all components
   - Integration tests for manipulation tasks
   - Real-world scenario testing

2. âœ… Documentation
   - API documentation
   - Tutorial notebooks
   - Video demonstrations
   - Troubleshooting guide

3. âœ… Optimization
   - Performance tuning
   - Reduce latency
   - Improve success rates

**Deliverables:**
- Complete test suite
- Full documentation
- Tutorial examples
- Performance report

---

## Safety Considerations

### Hardware Safety

1. **Joint Limits**
   ```python
   JOINT_LIMITS = {
       "shoulder_pan": (-170, 170),    # degrees
       "shoulder_lift": (-90, 90),
       "elbow": (0, 150),
       # ... more joints
   }
   ```
   - Enforce software limits before sending commands
   - Add buffer zone (5-10Â°) from mechanical limits
   - Monitor for limit violations

2. **Velocity Limits**
   ```python
   MAX_JOINT_VELOCITY = 50  # degrees/second
   MAX_GRIPPER_FORCE = 10   # Newtons
   ```
   - Limit maximum joint speeds
   - Ramp up/down for smooth motion
   - Reduce speed near limits

3. **Collision Detection**
   - Check self-collision (arm-to-arm, arm-to-body)
   - Monitor for unexpected resistance
   - Implement emergency stop on collision

4. **Emergency Stop**
   ```python
   def emergency_stop(self):
       """Immediately halt all arm motion."""
       self.left_arm_bus.disable_torque()
       self.right_arm_bus.disable_torque()
       self.gripper_bus.disable_torque()
   ```

### Software Safety

1. **Input Validation**
   - Validate all tool parameters
   - Check for reasonable values
   - Reject obviously unsafe commands

2. **State Monitoring**
   - Track current arm state
   - Detect stuck motors
   - Monitor power consumption

3. **Fallback Behaviors**
   - Retry logic for failed actions
   - Safe defaults for unclear situations
   - Graceful degradation

4. **Human-in-the-Loop**
   - Optional confirmation for risky actions
   - Manual override capability
   - Clear status feedback

### LLM Safety

1. **Tool Usage Constraints**
   - Limit force parameters
   - Restrict workspace zones
   - Require confirmation for critical actions

2. **Action Validation**
   ```python
   def validate_manipulation_action(action: Dict) -> bool:
       """Validate LLM-requested manipulation is safe."""
       # Check workspace bounds
       # Verify object is graspable
       # Ensure no humans in workspace
       return is_safe
   ```

3. **Monitoring**
   - Log all manipulation attempts
   - Track success/failure rates
   - Alert on unusual patterns

---

## Testing Strategy

### Unit Tests

**Location**: `tests/unit/`

1. **Arm Controller Tests**
   ```python
   def test_joint_position_control():
       """Test setting individual joint positions."""
       
   def test_gripper_control():
       """Test opening and closing gripper."""
       
   def test_joint_limits():
       """Test that limits are enforced."""
   ```

2. **Kinematics Tests**
   ```python
   def test_forward_kinematics():
       """Test FK calculation accuracy."""
       
   def test_inverse_kinematics():
       """Test IK solver correctness."""
       
   def test_workspace_boundaries():
       """Test reachability checking."""
   ```

3. **Tool Tests**
   ```python
   def test_reach_object_tool():
       """Test reach_object tool execution."""
       
   def test_pick_and_place_tool():
       """Test complete pick-place sequence."""
   ```

### Integration Tests

**Location**: `tests/integration/`

1. **Hardware Integration**
   - Test with actual robot hardware
   - Verify motor communication
   - Validate sensor readings

2. **VLA Integration**
   - Test VLA model loading
   - Verify action prediction
   - Benchmark inference time

3. **End-to-End Tests**
   ```python
   def test_pick_up_object():
       """Test complete object pickup workflow."""
       agent.task = "Pick up the red cup"
       result = agent.go()
       assert "success" in result.lower()
   ```

### Simulation Testing

**Location**: `tests/simulation/`

1. **Physics Simulation**
   - Use PyBullet or MuJoCo
   - Test in virtual environment first
   - Validate before hardware deployment

2. **Scenario Testing**
   - Common household tasks
   - Edge cases and failures
   - Multi-step manipulation sequences

### Performance Benchmarks

**Metrics to Track:**
1. **Success Rate**: % of successful manipulations
2. **Execution Time**: Time from command to completion
3. **Precision**: Distance from target position
4. **Safety Events**: Number of collisions/violations
5. **VLA Inference Time**: Model prediction latency

**Target Benchmarks:**
- Success rate: >85% for common objects
- Execution time: <10 seconds for simple grasp
- Precision: <2cm position error
- Safety events: 0 collisions
- VLA inference: <500ms

---

## Implementation Details

### Configuration File Structure

**File**: `src/robocrew/robots/XLeRobot/arm_config.json`

```json
{
  "left_arm": {
    "usb_port": "/dev/arm_left",
    "motors": {
      "1": {
        "name": "shoulder_pan",
        "type": "sts3215",
        "limits": [-170, 170],
        "home_position": 0,
        "max_velocity": 50
      },
      "2": {
        "name": "shoulder_lift",
        "type": "sts3215",
        "limits": [-90, 90],
        "home_position": 0,
        "max_velocity": 50
      },
      "3": {
        "name": "elbow",
        "type": "sts3215",
        "limits": [0, 150],
        "home_position": 90,
        "max_velocity": 60
      },
      "4": {
        "name": "wrist_rotate",
        "type": "sts3215",
        "limits": [-180, 180],
        "home_position": 0,
        "max_velocity": 80
      },
      "5": {
        "name": "wrist_tilt",
        "type": "sts3215",
        "limits": [-90, 90],
        "home_position": 0,
        "max_velocity": 80
      },
      "6": {
        "name": "gripper",
        "type": "sts3215",
        "limits": [0, 100],
        "home_position": 50,
        "max_velocity": 100
      }
    }
  },
  "right_arm": {
    "usb_port": "/dev/arm_right",
    "motors": {
      // Similar configuration
    }
  },
  "dh_parameters": {
    "left_arm": [
      // Denavit-Hartenberg parameters for FK/IK
    ]
  }
}
```

### Example Usage Script

**File**: `examples/object_manipulation_demo.py`

```python
from robocrew.core.LLMAgent import LLMAgent
from robocrew.core.tools import finish_task
from robocrew.robots.XLeRobot.wheel_controls import XLeRobotWheels
from robocrew.robots.XLeRobot.arm_controls import XLeRobotArms
from robocrew.robots.XLeRobot.tools import (
    create_move_forward, create_turn_left, create_turn_right
)
from robocrew.robots.XLeRobot.manipulation_tools import (
    create_reach_object,
    create_grasp_object,
    create_release_object,
    create_pick_and_place,
    create_reset_arm
)
from robocrew.models.vla import VLAController

# Initialize hardware
wheel_controller = XLeRobotWheels(wheel_arm_usb="/dev/arm_right")
arm_controller = XLeRobotArms(
    left_arm_usb="/dev/arm_left",
    right_arm_usb="/dev/arm_right"
)

# Initialize VLA model (optional)
vla_controller = VLAController(model_name="openvla-7b")

# Create movement tools
move_forward = create_move_forward(wheel_controller)
turn_left = create_turn_left(wheel_controller)
turn_right = create_turn_right(wheel_controller)

# Create manipulation tools
reach_object = create_reach_object(arm_controller, vla_controller)
grasp_object = create_grasp_object(arm_controller)
release_object = create_release_object(arm_controller)
pick_and_place = create_pick_and_place(arm_controller, vla_controller)
reset_arm = create_reset_arm(arm_controller)

# Create agent with full capabilities
agent = LLMAgent(
    model="google_genai:gemini-robotics-er-1.5-preview",
    tools=[
        # Movement
        move_forward, turn_left, turn_right,
        # Manipulation
        reach_object, grasp_object, release_object,
        pick_and_place, reset_arm,
        # Control
        finish_task
    ],
    main_camera_usb_port="/dev/video0",
    camera_fov=120,
)

# Give it a manipulation task
agent.task = "Find the blue cup on the table and bring it to me"
agent.go()

# Cleanup
arm_controller.disconnect()
wheel_controller.disconnect()
```

### README Update

Add to the Features section:
```markdown
- ðŸ¦¾ **Manipulation** - VLA-powered arm control for object interaction
  - Intelligent grasping with vision-language-action models
  - Dual arm coordination
  - Object detection and tracking
  - Pick and place operations
```

Add a new section:
```markdown
### Object Manipulation (XLeRobot)

```python
from robocrew.robots.XLeRobot.arm_controls import XLeRobotArms
from robocrew.robots.XLeRobot.manipulation_tools import (
    create_reach_object, create_grasp_object, create_pick_and_place
)
from robocrew.models.vla import VLAController

# Set up arm control
arm_controller = XLeRobotArms(
    left_arm_usb="/dev/arm_left",
    right_arm_usb="/dev/arm_right"
)

# Initialize VLA model for intelligent manipulation
vla_controller = VLAController(model_name="openvla-7b")

# Create manipulation tools
reach_object = create_reach_object(arm_controller, vla_controller)
grasp_object = create_grasp_object(arm_controller)
pick_and_place = create_pick_and_place(arm_controller, vla_controller)

# Add to your agent
agent = LLMAgent(
    model="google_genai:gemini-robotics-er-1.5-preview",
    tools=[reach_object, grasp_object, pick_and_place, ...],
    ...
)

agent.task = "Pick up the red apple and place it in the basket"
agent.go()
```
```

---

## Future Enhancements

### Short-term (3-6 months)

1. **Bimanual Coordination**
   - Synchronize both arms for complex tasks
   - Implement handover between arms
   - Add two-handed manipulation primitives

2. **Force Control**
   - Add force/torque sensing
   - Implement compliant grasping
   - Enable tactile feedback

3. **Advanced Perception**
   - Depth-based object localization
   - 3D point cloud processing
   - Object pose estimation

4. **More VLA Models**
   - Support multiple VLA architectures
   - Model fine-tuning capabilities
   - Custom policy training

### Long-term (6-12 months)

1. **Learning from Demonstration**
   - Record and replay manipulation sequences
   - Fine-tune VLA models on custom tasks
   - Interactive learning interface

2. **Multi-modal Feedback**
   - Haptic feedback integration
   - Audio cues for grasping
   - Visual servoing for precision

3. **Task Libraries**
   - Pre-built manipulation primitives
   - Common household task templates
   - Modular task composition

4. **Sim-to-Real Transfer**
   - Train in simulation first
   - Automated policy adaptation
   - Reality gap bridging techniques

5. **Multi-Robot Coordination**
   - Coordinate multiple robots
   - Shared manipulation tasks
   - Distributed task planning

---

## Dependencies and Requirements

### Hardware Requirements

- **XLeRobot or compatible platform** with:
  - 2x robotic arms (6 DOF each recommended)
  - Grippers with position/force control
  - USB serial interfaces for each arm
  - Camera with depth sensing (optional but recommended)

### Software Requirements

**Core Dependencies** (already installed):
- Python 3.10+
- lerobot library
- langchain and langchain-core
- opencv-python
- pyserial
- feetech-servo-sdk

**New Dependencies**:
```python
# Add to requirements.txt
numpy>=1.24.0
scipy>=1.10.0           # For IK solver
torch>=2.0.0            # For VLA models
transformers>=4.30.0    # For VLA models
pillow>=9.5.0          # Image processing
pybullet>=3.2.5        # Optional: for simulation
```

**Optional Dependencies**:
```python
# For advanced features
opencv-contrib-python  # For advanced vision
ultralytics>=8.0.0    # For YOLOv8 object detection
open3d>=0.17.0        # For point cloud processing
rtabmap>=0.21.0       # For 3D mapping
```

### Model Downloads

**VLA Models**:
- OpenVLA-7B: ~14GB download
- RT-2-X: ~7GB download
- Custom LeRobot policies: Variable size

**Object Detection Models**:
- YOLOv8n: ~6MB
- YOLOv8m: ~50MB

---

## Risk Assessment and Mitigation

### Technical Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| VLA model too slow | High | Medium | Use quantized models, GPU acceleration |
| Poor manipulation success rate | High | Medium | Extensive testing, fallback to simpler methods |
| Motor communication issues | High | Low | Robust error handling, connection monitoring |
| IK solver failures | Medium | Medium | Multiple IK solutions, fallback to teaching |
| Object detection errors | Medium | High | Multi-modal detection, confidence thresholds |

### Hardware Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Arm collision damage | High | Medium | Collision detection, software limits, testing |
| Gripper damage | Medium | Low | Force limits, soft grippers, careful testing |
| Motor overheating | Medium | Low | Duty cycle limits, temperature monitoring |
| Power supply issues | High | Low | Proper power management, monitoring |

### Project Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Timeline delays | Medium | Medium | Phased approach, MVP focus |
| Scope creep | Medium | High | Clear requirements, phase gates |
| Integration issues | High | Medium | Early integration testing, modular design |
| Documentation gaps | Low | High | Continuous documentation, code reviews |

---

## Success Metrics

### Quantitative Metrics

1. **Manipulation Success Rate**: >85% for common objects
2. **Average Execution Time**: <15 seconds per grasp
3. **Position Accuracy**: <2cm error
4. **System Latency**: <1 second from decision to action
5. **Safety**: 0 hardware collisions in testing
6. **Code Coverage**: >80% test coverage

### Qualitative Metrics

1. **Usability**: Easy to add new manipulation tasks
2. **Robustness**: Handles failures gracefully
3. **Documentation**: Clear and comprehensive
4. **Maintainability**: Clean, modular code
5. **Extensibility**: Easy to add new robots/capabilities

### Milestone Gates

**Phase 1 Complete**: When basic arm control works reliably
**Phase 2 Complete**: When LLM can command arm movements
**Phase 3 Complete**: When VLA-guided manipulation succeeds >70%
**Phase 4 Complete**: When trajectory planning prevents collisions
**Phase 5 Complete**: When documentation and tests are complete

---

## Conclusion

This integration plan provides a comprehensive roadmap for adding object manipulation capabilities to RoboCrew. By following this phased approach, we can:

1. **Leverage existing infrastructure** (lerobot, tool architecture)
2. **Maintain consistency** with current design patterns
3. **Ensure safety** through multiple layers of protection
4. **Enable intelligent manipulation** via VLA models
5. **Provide flexibility** for various manipulation tasks
6. **Support future enhancements** through modular design

The key to success is:
- **Start simple**: Get basic arm control working first
- **Test extensively**: Both in simulation and with real hardware
- **Prioritize safety**: Multiple safety layers
- **Document thoroughly**: Make it easy for others to use
- **Iterate quickly**: Learn from failures and improve

With this plan, RoboCrew will gain powerful manipulation capabilities while maintaining its core philosophy of making robot programming simple and accessible.

---

## Appendix

### A. Reference Architectures

- **LeRobot**: https://github.com/huggingface/lerobot
- **OpenVLA**: https://openvla.github.io/
- **RT-2**: https://robotics-transformer2.github.io/
- **PyBullet**: https://pybullet.org/

### B. Motor Specifications

- **Feetech STS3215**: 
  - Protocol: Serial bus protocol
  - Position resolution: 4096 steps
  - Velocity range: 0-10000 steps/s
  - Communication: UART, 1Mbps

### C. Coordinate Systems

```
Robot Base Frame:
- Origin: Center of robot base
- X: Forward
- Y: Left
- Z: Up

End-Effector Frame:
- Origin: Gripper center point
- X: Forward (toward grasp direction)
- Y: Left
- Z: Up
```

### D. Useful Commands

```bash
# List USB devices
ls /dev/tty* | grep -E "USB|ACM"

# Monitor serial communication
python -m serial.tools.miniterm /dev/ttyUSB0 1000000

# Test arm connection
python -c "from robocrew.robots.XLeRobot.arm_controls import XLeRobotArms; 
           arm = XLeRobotArms(); arm.test_connection()"

# Run manipulation demo
python examples/object_manipulation_demo.py
```

### E. Troubleshooting Guide

**Problem**: Motors not responding
- Check USB connections
- Verify motor IDs
- Check power supply
- Test with simple serial write

**Problem**: IK solver fails
- Check if target is reachable
- Verify joint limits
- Try different IK seeds

**Problem**: VLA model slow
- Use GPU if available
- Try quantized model
- Reduce image resolution

**Problem**: Poor grasp success
- Calibrate gripper force
- Improve object detection
- Adjust approach angle
- Fine-tune VLA model

---

## Document Metadata

- **Version**: 1.0
- **Last Updated**: 2025-11-22
- **Author**: RoboCrew Development Team
- **Status**: Draft
- **Review Date**: TBD
